{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11202d19",
   "metadata": {},
   "source": [
    "# Exploring Mental Health Data\n",
    "**Objective:** Predict whether an individual suffers from depression based on a set of responses from a mental health survey.\n",
    "\n",
    "**Problem task:** Binary classification on the target variable depression (0 = false, 1 = true)\n",
    "\n",
    "**Dataset source:** Kaggle - Playground Series S4E11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92346d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:58.277971Z",
     "start_time": "2025-05-12T14:02:57.989171Z"
    }
   },
   "outputs": [],
   "source": [
    "#Marta path:\n",
    "#Ricardo path:\n",
    "#Sara path: \"/Users/saracortez/feup/3o ano/iart/exploring_mental_health_data/data/train.csv\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "train_data = pd.read_csv(\"/Users/saracortez/feup/3o ano/iart/exploring_mental_health_data/data/train.csv\")\n",
    "test_data = pd.read_csv(\"/Users/saracortez/feup/3o ano/iart/exploring_mental_health_data/data/test.csv\")\n",
    "\n",
    "print(train_data.head())\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9efd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:58.482581Z",
     "start_time": "2025-05-12T14:02:58.346302Z"
    }
   },
   "outputs": [],
   "source": [
    "#duplicate removal\n",
    "bf = len(train_data)\n",
    "print(f\"Number of rows before removing duplicates: {len(train_data)}\")\n",
    "train_data = train_data.drop_duplicates()\n",
    "af = len(train_data)\n",
    "print(f\"Number of rows after removing duplicates: {len(train_data)}\")\n",
    "if (bf-af) == 0:\n",
    "    print(\"(No dup data found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6de5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:58.596244Z",
     "start_time": "2025-05-12T14:02:58.535537Z"
    }
   },
   "outputs": [],
   "source": [
    "#missing value check\n",
    "def missing_value_info(df):\n",
    "    total = df.isnull().sum()\n",
    "    percent = (total / len(df)) * 100\n",
    "    return pd.DataFrame({'Missing Values': total, 'Percent Missing': percent}).sort_values(by='Percent Missing', ascending=False)\n",
    "missing_info_with_0 = missing_value_info(train_data)\n",
    "missing_info = missing_info_with_0[missing_info_with_0['Percent Missing'] > 0.0]\n",
    "print(missing_info)\n",
    "print(missing_info_with_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5bd7f",
   "metadata": {},
   "source": [
    "since these columns were spotted for missing values, we want to understand their appearance: how many are missing (NaN count in value_counts), if there unexpected 0s or negative values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf28ee7",
   "metadata": {},
   "source": [
    "### Data prepp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e037ebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:58.685835Z",
     "start_time": "2025-05-12T14:02:58.658439Z"
    }
   },
   "outputs": [],
   "source": [
    "#train_data['Study Satisfaction'].value_counts(dropna=False)\n",
    "#train_data['Academic Pressure'].value_counts(dropna=False)\n",
    "#train_data['CGPA'].value_counts(dropna=False)\n",
    "#train_data['Profession'].value_counts(dropna=False)\n",
    "#train_data['Work Pressure'].value_counts(dropna=False)\n",
    "#train_data['Job Satisfaction'].value_counts(dropna=False)\n",
    "#train_data['Dietary Habits'].value_counts(dropna=False)\n",
    "#train_data['Financial Stress'].value_counts(dropna=False)\n",
    "pd.set_option('display.max_rows', None)\n",
    "train_data['Degree'].value_counts(dropna=False)\n",
    "#regulated\n",
    "#Comclusion: all our missing vals are NANS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873306d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forçar a visualização completa das contagens\n",
    "pd.set_option('display.max_rows', None)  # Isso vai permitir que todos os valores sejam exibidos\n",
    "print(train_data['Profession'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660de5c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:58.823839Z",
     "start_time": "2025-05-12T14:02:58.761838Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['Profession'].value_counts(dropna=False)\n",
    "\n",
    "valid_professions = [\n",
    "    \"Teacher\", \"Content Writer\", \"Architect\", \"Consultant\", \"HR Manager\",\n",
    "    \"Pharmacist\", \"Doctor\", \"Business Analyst\", \"Entrepreneur\", \"Chemist\",\n",
    "    \"Chef\", \"Educational Consultant\", \"Data Scientist\", \"Researcher\", \"Lawyer\",\n",
    "    \"Customer Support\", \"Marketing Manager\", \"Pilot\", \"Travel Consultant\",\n",
    "    \"Plumber\", \"Sales Executive\", \"Manager\", \"Judge\", \"Electrician\",\n",
    "    \"Financial Analyst\", \"Software Engineer\", \"Civil Engineer\", \"UX/UI Designer\",\n",
    "    \"Digital Marketer\", \"Accountant\", \"Mechanical Engineer\", \"Graphic Designer\",\n",
    "    \"Research Analyst\", \"Investment Banker\", \"Analyst\", \"Academic\", \"Unemployed\", \"Medical Doctor\", \"City Manager\", \"Family Consultant\"\n",
    "]\n",
    "\n",
    "# corrigir erros digitação\n",
    "def correct_profession(value):\n",
    "    corrections = {\n",
    "        \"Finanancial Analyst\": \"Financial Analyst\",\n",
    "        # Adicionar mais ??\n",
    "    }\n",
    "    return corrections.get(value, value)\n",
    "\n",
    "train_data['Profession'] = train_data['Profession'].apply(correct_profession)\n",
    "\n",
    "test_data['Profession'] = test_data['Profession'].apply(correct_profession)\n",
    "\n",
    "def clean_profession(value):\n",
    "    if pd.isna(value):\n",
    "        return value  # mantém NaN\n",
    "    return value if value in valid_professions else \"other\"\n",
    "\n",
    "train_data['Profession'] = train_data['Profession'].apply(clean_profession)\n",
    "test_data['Profession'] = test_data['Profession'].apply(clean_profession)\n",
    "\n",
    "train_data['Profession'].value_counts(dropna=False)\n",
    "\n",
    "test_data['Profession'].value_counts(dropna=False)\n",
    "\n",
    "# these are names: [\"Yogesh\", \"Pranav\", \"Dev\", \"Yuvraj\"]\n",
    "# these seem to be localities [\"Patna\", \"Visakhapatnam\", \"Nagpur\", \"FamilyVirar\"]\n",
    "# and these ? what are hey ? not jobs. [\"Patna\", \"Visakhapatnam\", \"Nagpur\", \"FamilyVirar\"]\n",
    "#degrees like MBA\n",
    "#substringing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a72413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:59.400537Z",
     "start_time": "2025-05-12T14:02:58.906575Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# handle ranges like 6-8, handle more than/less than X,handle direct numeric values\n",
    "print(len(train_data['Sleep Duration']))\n",
    "#15 rows lost\n",
    "def normalize_sleep_duration(column):\n",
    "    def normalize(value):\n",
    "        value = str(value).strip()\n",
    "\n",
    "        match_range = re.match(r\"(\\d+)\\s*-\\s*(\\d+)\", value)\n",
    "        if match_range:\n",
    "            x, y = map(int, match_range.groups())\n",
    "            return (x + y) / 2\n",
    "\n",
    "        match_more = re.match(r\"More than (\\d+)\", value, re.IGNORECASE)\n",
    "        if match_more:\n",
    "            return int(match_more.group(1)) + 0.5\n",
    "\n",
    "        match_less = re.match(r\"Less than (\\d+)\", value, re.IGNORECASE)\n",
    "        if match_less:\n",
    "            return int(match_less.group(1)) - 0.5\n",
    "\n",
    "        try:\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            return pd.NA \n",
    "\n",
    "    return column.apply(normalize)\n",
    "\n",
    "def normalize_large_sleep_values(column):\n",
    "    def adjust_large(value):\n",
    "        try:\n",
    "            if pd.notna(value) and value >= 12:\n",
    "                return round(value / 7 * 2) / 2\n",
    "            return value\n",
    "        except:\n",
    "            return pd.NA\n",
    "    return column.apply(adjust_large)\n",
    "\n",
    "train_data['Sleep Duration'] = normalize_sleep_duration(train_data['Sleep Duration'])\n",
    "train_data['Sleep Duration'] = normalize_large_sleep_values(train_data['Sleep Duration'])\n",
    "test_data['Sleep Duration'] = normalize_sleep_duration(test_data['Sleep Duration'])\n",
    "test_data['Sleep Duration'] = normalize_large_sleep_values(test_data['Sleep Duration'])\n",
    "\n",
    "print(train_data['Sleep Duration'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d35ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:59.407590647Z",
     "start_time": "2025-05-12T13:59:33.294320Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_dietary = [\"Moderate\", \"Unhealthy\", \"Healthy\"]\n",
    "\n",
    "train_data[\"Dietary Habits\"] = train_data[\"Dietary Habits\"].apply(\n",
    "    lambda x: x if pd.isna(x) or x in valid_dietary else \"other\"\n",
    ")\n",
    "test_data[\"Dietary Habits\"] = test_data[\"Dietary Habits\"].apply(\n",
    "    lambda x: x if pd.isna(x) or x in valid_dietary else \"other\"\n",
    ")\n",
    "\n",
    "\n",
    "print(train_data['Dietary Habits'].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_degree(column):\n",
    "    def clean(item):\n",
    "        if isinstance(item, str):\n",
    "            item = item.replace('.', '').replace(' ', '')\n",
    "            return item\n",
    "        else:\n",
    "            return 'invalid'\n",
    "    \n",
    "    column = column.apply(clean)\n",
    "    def remove_names(item):\n",
    "            if (len(item) > 1 and item[0].isupper() and item[1].isupper() and item[0] in ['L', 'P', 'B', 'M']) or item == 'Class12' or item == \"PhD\":\n",
    "                return item\n",
    "            else:\n",
    "                return 'invalid' \n",
    "    return column.apply(remove_names)\n",
    "#importante dar NA aos inválidos para dar drop\n",
    "train_data['Degree'] = normalize_degree(train_data['Degree'])\n",
    "test_data['Degree'] = normalize_degree(test_data['Degree'])\n",
    "\n",
    "degree_counts = train_data['Degree'].value_counts()\n",
    "rare_degrees = degree_counts[degree_counts <= 5].index\n",
    "\n",
    "train_data['Degree'] = train_data['Degree'].apply(lambda x: 'other' if x in rare_degrees else x)\n",
    "test_data['Degree'] = test_data['Degree'].apply(lambda x: 'other' if x in rare_degrees else x)\n",
    "     \n",
    "\n",
    "print(train_data['Degree'].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d23e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data['Work/Study Hours'].value_counts(dropna=False))\n",
    "\"\"\"\n",
    "train_data['Work/Study Hours'].value_counts(dropna=False)\n",
    "train_data['Academic Pressure'].value_counts(dropna=False)\n",
    "train_data['CGPA'].value_counts(dropna=False)\n",
    "train_data['Profession'].value_counts(dropna=False)\n",
    "train_data['Work Pressure'].value_counts(dropna=False)\n",
    "train_data['Job Satisfaction'].value_counts(dropna=False)\n",
    "train_data['Dietary Habits'].value_counts(dropna=False)\n",
    "train_data['Financial Stress'].value_counts(dropna=False)\n",
    "train_data['Degree'].value_counts(dropna=False)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec77f77",
   "metadata": {},
   "source": [
    "Our decision tree classifier requires binary values. Thus, let's convert bicategorical variables in to 0/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gender column: Male -> 1, Female -> 0\n",
    "train_data['Gender'] = train_data['Gender'].replace({'Male': 1, 'Female': 0})\n",
    "\n",
    "# Working Professional or Student column: Working Professional -> 1, Student -> 0\n",
    "train_data['Working Professional or Student'] = train_data['Working Professional or Student'].replace({'Working Professional': 1, 'Student': 0})\n",
    "\n",
    "# Have you ever had suicidal thoughts?\n",
    "train_data['Have you ever had suicidal thoughts ?'] = train_data['Have you ever had suicidal thoughts ?'].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "train_data['Family History of Mental Illness'] = train_data['Family History of Mental Illness'].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "test_data['Gender'] = test_data['Gender'].replace({'Male': 1, 'Female': 0})\n",
    "\n",
    "test_data['Working Professional or Student'] = test_data['Working Professional or Student'].replace({'Working Professional': 1, 'Student': 0})\n",
    "\n",
    "test_data['Have you ever had suicidal thoughts ?'] = test_data['Have you ever had suicidal thoughts ?'].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "test_data['Family History of Mental Illness'] = test_data['Family History of Mental Illness'].replace({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf312e4",
   "metadata": {},
   "source": [
    "## Merging Columns\n",
    "\n",
    "we can detect in our data two categories of individuals, identifiable by their attributes:\n",
    "- **Students:** academic pressure, CGPA, study satisfaction, degree\n",
    "- **Worker Professionals:** work pressure, profession, job satisfaction\n",
    "\n",
    "So we decided to merge the columns that mean the same thing relatively yo each of these categories.\n",
    "\n",
    "For example: 'Job Satisfaction' and 'Study Satisfaction' merge into just 'Satisfaction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b9fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Satisfaction'] = train_data[['Job Satisfaction', 'Study Satisfaction']].mean(axis=1, skipna=True)\n",
    "\n",
    "train_data = train_data.drop(columns=['Job Satisfaction', 'Study Satisfaction'])\n",
    "train_data['Pressure'] = train_data[['Work Pressure', 'Academic Pressure']].mean(axis=1, skipna=True)\n",
    "\n",
    "train_data = train_data.drop(columns=['Work Pressure', 'Academic Pressure'])\n",
    "print(train_data[['Pressure']].head())\n",
    "\n",
    "print(train_data[['Satisfaction']].head())\n",
    "\n",
    "train_data.loc[\n",
    "    train_data['Profession'].isna() & (train_data['Working Professional or Student'] == 0),\n",
    "    'Profession'\n",
    "] = 'Student'\n",
    "\n",
    "train_data.loc[\n",
    "    train_data['Profession'].isna() & (train_data['Working Professional or Student'] != 0),\n",
    "    'Profession'\n",
    "] = 'other'\n",
    "train_data = train_data.drop(columns=['CGPA'])\n",
    "print(train_data[['Profession']].head())\n",
    "\n",
    "test_data['Satisfaction'] = test_data[['Job Satisfaction', 'Study Satisfaction']].mean(axis=1, skipna=True)\n",
    "\n",
    "test_data = test_data.drop(columns=['Job Satisfaction', 'Study Satisfaction'])\n",
    "test_data['Pressure'] = test_data[['Work Pressure', 'Academic Pressure']].mean(axis=1, skipna=True)\n",
    "\n",
    "test_data = test_data.drop(columns=['Work Pressure', 'Academic Pressure'])\n",
    "\n",
    "\n",
    "test_data.loc[\n",
    "    test_data['Profession'].isna() & (test_data['Working Professional or Student'] == 0),\n",
    "    'Profession'\n",
    "] = 'Student'\n",
    "\n",
    "test_data.loc[\n",
    "    test_data['Profession'].isna() & (test_data['Working Professional or Student'] != 0),\n",
    "    'Profession'\n",
    "] = 'other'\n",
    "test_data = test_data.drop(columns=['CGPA'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106440f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"train_merge.csv\", index=False)\n",
    "test_data.to_csv(\"test_merge.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23999e1c",
   "metadata": {},
   "source": [
    "##  Undersampling with priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cece835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDERSAMPLING COM PRIORIDADE\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "X = train_data.drop(\"Depression\", axis=1)\n",
    "y = train_data[\"Depression\"]\n",
    "\n",
    "data = pd.concat([X, y], axis=1)\n",
    "\n",
    "class_counts = data[\"Depression\"].value_counts()\n",
    "min_class_size = class_counts.min()\n",
    "\n",
    "priority_cols = [\"Profession\", \"Degree\", \"Dietary Habits\"]\n",
    "\n",
    "balanced_data = []\n",
    "\n",
    "for label in class_counts.index:\n",
    "    subset = data[data[\"Depression\"] == label]\n",
    "\n",
    "    if len(subset) > min_class_size:\n",
    "        \n",
    "        to_keep = min_class_size\n",
    "        \n",
    "        na_rows = subset[priority_cols].isna().any(axis=1)\n",
    "        subset = subset[~na_rows]\n",
    "\n",
    "        # Primeira prioridade: linhas com \"invalid\"\n",
    "        invalid_rows = subset[subset[priority_cols].isin([\"invalid\"]).any(axis=1)]\n",
    "        subset = subset.drop(invalid_rows.index)\n",
    "\n",
    "        # Segunda prioridade: linhas com \"other\"\n",
    "        other_rows = subset[subset[priority_cols].isin([\"other\"]).any(axis=1)]\n",
    "        subset = subset.drop(other_rows.index)\n",
    "\n",
    "        remaining_needed = to_keep\n",
    "\n",
    "\n",
    "        if len(subset) >= remaining_needed:\n",
    "            to_sample = subset.sample(remaining_needed, random_state=42)\n",
    "        else:\n",
    "            # Remover todos os 'priority' e sortear os restantes\n",
    "            rows_needed = remaining_needed - len(subset)\n",
    "\n",
    "            if len(other_rows) >= rows_needed:\n",
    "                to_sample = pd.concat([subset, other_rows.sample(rows_needed, random_state=42)])\n",
    "            else:\n",
    "                still_needed = rows_needed - len(other_rows)\n",
    "                to_sample = pd.concat([\n",
    "                    subset,\n",
    "                    other_rows,\n",
    "                    invalid_rows.sample(still_needed, random_state=42)\n",
    "                ])\n",
    "    else:\n",
    "\n",
    "        to_sample = subset\n",
    "\n",
    "    balanced_data.append(to_sample)\n",
    "\n",
    "undersampled_data = pd.concat(balanced_data)\n",
    "\n",
    "undersampled_data.to_csv(\"final_train_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = undersampled_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1588c2",
   "metadata": {},
   "source": [
    "Após o undersampling por forma a normalizar a razão No depression/Depression nos dados de teste, o número de casos ficou:\n",
    "\n",
    "No depression: 5084\n",
    "\n",
    "Depression: 5143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"final_train_dataset.csv\")\n",
    "print(df.columns)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "data = df.copy()\n",
    "data_test = test_data.copy()\n",
    "\n",
    "categorical_cols_train = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "\n",
    "# transforma as colunas com \"palavras\" em numeros\n",
    "label_encoders = {}\n",
    "for col in categorical_cols_train:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "for col in categorical_cols_train:\n",
    "    if col in data_test.columns:\n",
    "        le = label_encoders[col]\n",
    "        data_test[col] = data_test[col].apply(lambda x: x if x in le.classes_ else 'unknown')\n",
    "\n",
    "        if 'unknown' not in le.classes_:\n",
    "            le.classes_ = np.append(le.classes_, 'unknown')\n",
    "\n",
    "        data_test[col] = le.transform(data_test[col].astype(str))\n",
    "\n",
    "\n",
    "X_train = data.drop(columns='Depression')\n",
    "y_train = data['Depression']\n",
    "\n",
    "X_test = data_test[X_train.columns]\n",
    "\n",
    "\n",
    "# Tratar valores nulos (usar média quando tem valores nulos)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_data_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "\n",
    "\n",
    "#DUVIDA do que fazer aqui\n",
    "#apaga as linhas do teste com coluna nao numericas\n",
    "#mask = X_test.applymap(lambda x: isinstance(x, (int, float, np.number))).all(axis=1)\n",
    "#X_test_numeric = X_test[mask]\n",
    "\n",
    "#ALTERNATIVA:\n",
    "# Converter todas as colunas para numérico\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "test_data_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "\n",
    "drop_cols = ['id', 'Name', 'City']\n",
    "train_data_clean = train_data_imputed.drop(columns=drop_cols, errors='ignore')\n",
    "test_data_clean = test_data_imputed.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_trainSplit, X_testSplit, y_trainSplit, y_testSplit = train_test_split(train_data_clean, y_train, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b578e",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b1782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# I have tried other kernels (linear, poly and sigmoid) and they all perform similarly, the sigmoid being the worst\n",
    "model = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1, gamma='scale', probability=True))\n",
    "model.fit(X_trainSplit, y_trainSplit)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to train the model: {end_time - start_time:.2f} seconds\") #when probability is true (to collect data), it takes ~5 times longer to train\n",
    "\n",
    "y_pred = model.predict(X_testSplit)\n",
    "conf_matrix = confusion_matrix(y_testSplit, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_testSplit, y_pred))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['No Depression', 'Depression'],\n",
    "            yticklabels=['No Depression', 'Depression'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - SVM')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "y_pred_proba = model.predict_proba(X_testSplit)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_testSplit, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic - SVM')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_testSplit, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a3c8d",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0858db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "sb.scatterplot(x='Work/Study Hours', y='Sleep Duration',\n",
    "                hue='Have you ever had suicidal thoughts ?', data=train_data_clean)\n",
    "plt.title('Work Hours vs. Sleep Duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1b45e",
   "metadata": {},
   "source": [
    "ALGORITHMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a030350",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(train_data_clean[[\n",
    "    'Satisfaction', 'Sleep Duration', 'Work/Study Hours',\n",
    "    'Have you ever had suicidal thoughts ?'\n",
    "]], hue='Have you ever had suicidal thoughts ?')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09806c26",
   "metadata": {},
   "source": [
    "K-NEIGHBOUR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e44125",
   "metadata": {},
   "source": [
    "Para este algortimo é necessário que os dados estejam padronizados para que nenhuma variável influencie desproporcionalmente o cálculo da distância. Isso melhora a performance e a precisão do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22003e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Padronizar os dados (por tudo na mesma escala : nao pode idade(0-100) e genero(0 ou 1), é preciso escalar)\n",
    "\n",
    "#Este é com o split para fazermos as métricas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_trainSplit)\n",
    "X_test_scaled = scaler.transform(X_testSplit)\n",
    "\n",
    "\n",
    "# Este é para os dados de teste real que queremos tentar prever\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_real = scaler.fit_transform(train_data_imputed)\n",
    "X_test_scaled_real = scaler.transform(test_data_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "start_time = time.time()\n",
    "# Instanciar e treinar o modelo\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(X_train_scaled, y_trainSplit)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to train the model: {end_time - start_time:.2f} seconds\") #when probability is true (to collect data), it takes ~5 times longer to train\n",
    "\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Avaliar o desempenho\n",
    "accuracy = accuracy_score(y_testSplit, y_pred)\n",
    "conf_matrix = confusion_matrix(y_testSplit, y_pred)\n",
    "class_report = classification_report(y_testSplit, y_pred)\n",
    "\n",
    "class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025887bd",
   "metadata": {},
   "source": [
    "Através da análise dos resultados obtidos concluímos que os valores de precision,  recall e F1-score indicam que o modelo tem boa capacidade de prever corretamente se o indivíduo tem ou não depressão, não havendo muitos falsos positivos nem falsos negativos e que não favorece uma mais classe mais do que a outra. Além disso, as médias macro e ponderada das métricas também ficaram em 91%, o que reforça o equilíbrio do desempenho do modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124bebf",
   "metadata": {},
   "source": [
    "graphics / confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fa177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Depression', 'Depression'],\n",
    "            yticklabels=['No Depression', 'Depression'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - KNN')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "y_prob = knn.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_testSplit, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=f'KNN (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - KNN')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d792e397",
   "metadata": {},
   "source": [
    "A confusion matrix e o gráfico da ROC curve vêm a reforçar aquilo que as metricas nos permitiram concluir. A confusion matrix permite-nos concluir o seguinte:\n",
    "\n",
    "- Verdadeiros Positivos (TP): 4639 – Casos corretamente classificados como com depressão.\n",
    "\n",
    "- Verdadeiros Negativos (TN): 4705 – Casos corretamente classificados como sem depressão.\n",
    "\n",
    "- Falsos Positivos (FP): 379 – Casos sem depressão classificados erroneamente como com depressão.\n",
    "\n",
    "- Falsos Negativos (FN): 504 – Casos com depressão classificados erroneamente como sem depressão.\n",
    "\n",
    "Esses valores mostram que o modelo está bom tanto para identificar corretamente quem tem depressão quanto quem não tem. A maioria dos casos foi classificada corretamente.\n",
    "\n",
    "A ROC curve mostra a relação entre a taxa de verdadeiros positivos e a taxa de falsos positivos. A Área sob a Curva (AUC) é 0.96, o que indica excelente capacidade discriminativa do modelo — ou seja, ele consegue distinguir muito bem entre as classes com e sem depressão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb71635",
   "metadata": {},
   "source": [
    "Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Define o número de divisões de treino/teste\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Gera os valores de treino e validação para diferentes tamanhos de treino\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    knn, X_train_scaled, y_trainSplit, cv=cv,\n",
    "    scoring='accuracy', train_sizes=np.linspace(0.1, 0.8, 5), n_jobs=1\n",
    ")\n",
    "\n",
    "# Calcula média e desvio padrão\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_sizes, train_mean, 'o-', label='Training Accuracy')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)\n",
    "plt.plot(train_sizes, test_mean, 'o-', label='Validation Accuracy')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2)\n",
    "plt.title('Learning Curve - KNN')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07e6aa",
   "metadata": {},
   "source": [
    "O modelo está a obter valores maior de accuracy com mais dados, tanto em treino quanto em validação.\n",
    "\n",
    "Existe uma pequena diferença (~2%) entre treino e validação, o que é normal para KNN e indica leve overfitting.\n",
    "\n",
    "As curvas não estão a convergir totalemnte totalmente o que pode indicar que o modelo ainda não atingiu o seu limite de aprendizado e que talvez, com mais dados ou com ajustes no modelo (tal como o valor de k), o desempenho de validação possa melhorar ainda mais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20deb74",
   "metadata": {},
   "source": [
    "KNN Accuracy para diferentes valores de k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2bbb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Testar vários valores de k (1 a 20)\n",
    "k_values = range(1, 11)\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_k = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_k.fit(X_train_scaled, y_trainSplit)\n",
    "    y_pred_k = knn_k.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_testSplit, y_pred_k)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "# Plot da acurácia vs. número de vizinhos (k)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, accuracies, marker='o')\n",
    "plt.title('KNN Accuracy for Different Values of k')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0689746a",
   "metadata": {},
   "source": [
    "Através da análise do gráfico, podemos concluir que para valores de k muito baixos (como 1 ou 2), a acurácia também é mais reduzida. Isso pode ser explicado pelo fato de o modelo estar muito sensível ao overfitting, ajustando-se demais aos dados de treino.\n",
    "A partir de k=3 o valor de accuracy aumenta significativamente e estabiliza a partir de k=5 , o que nos levou a utilizar este valor de k no nosso modelo. Contudo, o melhor desempenho está entre k = 9 e k = 10, com acurácia próxima de 0.917.\n",
    "\n",
    "O modelo KNN melhora seu desempenho conforme k aumenta até cerca de 9 ou 10, o que indica que usar mais vizinhos ajuda a suavizar o efeito de possíveis outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d540811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calcular métricas individualmente\n",
    "precision_0 = precision_score(y_testSplit, y_pred, pos_label=0)\n",
    "recall_0 = recall_score(y_testSplit, y_pred, pos_label=0)\n",
    "f1_0 = f1_score(y_testSplit, y_pred, pos_label=0)\n",
    "\n",
    "precision_1 = precision_score(y_testSplit, y_pred, pos_label=1)\n",
    "recall_1 = recall_score(y_testSplit, y_pred, pos_label=1)\n",
    "f1_1 = f1_score(y_testSplit, y_pred, pos_label=1)\n",
    "\n",
    "# Criar DataFrame com as métricas\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': ['No Depression (0)', 'Depression (1)'],\n",
    "    'Precision': [precision_0, precision_1],\n",
    "    'Recall': [recall_0, recall_1],\n",
    "    'F1-score': [f1_0, f1_1],\n",
    "    'Support': [conf_matrix[0, 0] + conf_matrix[0, 1], conf_matrix[1, 0] + conf_matrix[1, 1]]\n",
    "})\n",
    "\n",
    "print(metrics_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea155e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tentar adivinhar os reais\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Instanciar e treinar o modelo\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled_real, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = knn.predict(X_test_scaled_real)\n",
    "\n",
    "# Mostrar previsões\n",
    "print(\"Previsões do modelo:\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2953c",
   "metadata": {},
   "source": [
    "Decison Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d2fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import time\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "clf.fit(X_trainSplit, y_trainSplit)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to train the Decision Tree Classifier: {end_time - start_time:.2f} seconds\") #when probability is true (to collect data), it takes ~5 times longer to train\n",
    "\n",
    "y_pred = clf.predict(X_testSplit)\n",
    "\n",
    "cf_mx_clf = confusion_matrix(y_testSplit, y_pred)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "accuracy = accuracy_score(y_testSplit, y_pred)\n",
    "report = classification_report(y_testSplit, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion matrix\\n\", cf_mx_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f91fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cf_mx_clf, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Depression', 'Depression'],\n",
    "            yticklabels=['No Depression', 'Depression'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Decision Tree Classifier')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "y_prob = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_testSplit, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='purple', label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic - Decision Tree Classifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cf44f8",
   "metadata": {},
   "source": [
    "Relembremos o número de casos da amostra:\n",
    "\n",
    "No depression: 5084\n",
    "\n",
    "Depression: 5143\n",
    "\n",
    "A matriz de confusão permite-nos concluir o seguinte:\n",
    "\n",
    "- Verdadeiros Positivos (TP): 4526 – 88%\n",
    "\n",
    "- Verdadeiros Negativos (TN): 4452 - 88%\n",
    "\n",
    "- Falsos Positivos (FP): 632 – 12%\n",
    "\n",
    "- Falsos Negativos (FN): 617 – 12%\n",
    "\n",
    "O modelo está bom tanto para identificar corretamente quem tem depressão quanto quem não tem. A maioria dos casos foi classificada corretamente. Como esperávamos, o Decision Tree Classifier foi de todos o que produziu uma _accuracy_ mais baixa, consequência da sua simplicidade. Por outro lado, o tempo de treino do modelo fica nos 0.2 segundos, afirmando-se como a alternativa menos dispendiosa em termos computacionais\n",
    "\n",
    "A ROC curve mostra a relação entre a taxa de verdadeiros positivos e a taxa de falsos positivos. A Área sob a Curva (AUC) é 0.88, traduzindo a _accuracy_ do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01014f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    clf, X_train_scaled, y_trainSplit, cv=cv,\n",
    "    scoring='accuracy', train_sizes=np.linspace(0.1, 0.8, 5), n_jobs=1\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_sizes, train_mean, 'o-', label='Training Accuracy')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)\n",
    "plt.plot(train_sizes, test_mean, 'o-', label='Validation Accuracy')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2)\n",
    "plt.title('Learning Curve - Decision Tree Classifier')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79281db2",
   "metadata": {},
   "source": [
    "considerações training e validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#previsão dos dados de teste\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_trainSplit, y_trainSplit)\n",
    "\n",
    "y_pred = clf.predict(X_testSplit)\n",
    "\n",
    "print(\"Previsões:\")\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8cf56",
   "metadata": {},
   "source": [
    "NEURAL NETWORK (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd26b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "start_time = time.time()\n",
    "mlp.fit(X_trainSplit, y_trainSplit)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to train the MultiLayer Perceptron: {end_time - start_time:.2f} seconds\") #when probability is true (to collect data), it takes ~5 times longer to train\n",
    "\n",
    "y_pred = mlp.predict(test_data_imputed)\n",
    "\n",
    "cf_mx_mlp = confusion_matrix(y_testSplit, y_pred)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "accuracy = accuracy_score(y_testSplit, y_pred)\n",
    "report = classification_report(y_testSplit, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion matrix\\n\", cf_mx_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f2c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cf_mx_mlp, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Depression', 'Depression'],\n",
    "            yticklabels=['No Depression', 'Depression'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - MultiLayer Perceptron')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "y_prob = mlp.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_testSplit, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='green', label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic - MultiLayer Perceptron')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d292b45",
   "metadata": {},
   "source": [
    " considerações matrix e roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b90f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    mlp, X_train_scaled, y_trainSplit, cv=cv,\n",
    "    scoring='accuracy', train_sizes=np.linspace(0.1, 0.8, 5), n_jobs=1\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_sizes, train_mean, 'o-', label='Training Accuracy')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)\n",
    "plt.plot(train_sizes, test_mean, 'o-', label='Validation Accuracy')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2)\n",
    "plt.title('Learning Curve - Multi Layer Perceptron')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5175549d",
   "metadata": {},
   "source": [
    " considerações training e validation accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "mlp.fit(X_train_scaled_real, y_trainSplit)\n",
    "\n",
    "y_pred = mlp.predict(test_data_imputed)\n",
    "\n",
    "print(\"Previsões:\")\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
