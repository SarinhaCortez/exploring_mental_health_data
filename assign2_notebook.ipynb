{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11202d19",
   "metadata": {},
   "source": [
    "# Exploring Mental Health Data\n",
    "**Objective:** Predict whether an individual suffers from depression based on a set of responses from a mental health survey.\n",
    "\n",
    "**Problem task:** Binary classification on the target variable depression (0 = false, 1 = true)\n",
    "\n",
    "**Dataset source:** Kaggle - Playground Series S4E11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92346d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:58.277971Z",
     "start_time": "2025-05-12T14:02:57.989171Z"
    }
   },
   "outputs": [],
   "source": [
    "#Marta path:\n",
    "#Ricardo path:\n",
    "#Sara path: \"/Users/saracortez/feup/3o ano/iart/exploring_mental_health_data/data/train.csv\"\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(train_data.head())\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9efd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:58.482581Z",
     "start_time": "2025-05-12T14:02:58.346302Z"
    }
   },
   "outputs": [],
   "source": [
    "#duplicate removal\n",
    "bf = len(train_data)\n",
    "print(f\"Number of rows before removing duplicates: {len(train_data)}\")\n",
    "train_data = train_data.drop_duplicates()\n",
    "af = len(train_data)\n",
    "print(f\"Number of rows after removing duplicates: {len(train_data)}\")\n",
    "if (bf-af) == 0:\n",
    "    print(\"(No dup data found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6de5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:58.596244Z",
     "start_time": "2025-05-12T14:02:58.535537Z"
    }
   },
   "outputs": [],
   "source": [
    "#missing value check\n",
    "def missing_value_info(df):\n",
    "    total = df.isnull().sum()\n",
    "    percent = (total / len(df)) * 100\n",
    "    return pd.DataFrame({'Missing Values': total, 'Percent Missing': percent}).sort_values(by='Percent Missing', ascending=False)\n",
    "missing_info_with_0 = missing_value_info(train_data)\n",
    "missing_info = missing_info_with_0[missing_info_with_0['Percent Missing'] > 0.0]\n",
    "print(missing_info)\n",
    "print(missing_info_with_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5bd7f",
   "metadata": {},
   "source": [
    "since these columns were spotted for missing values, we want to understand their appearance: how many are missing (NaN count in value_counts), if there unexpected 0s or negative values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf28ee7",
   "metadata": {},
   "source": [
    "### Data prepp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e037ebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:58.685835Z",
     "start_time": "2025-05-12T14:02:58.658439Z"
    }
   },
   "outputs": [],
   "source": [
    "#train_data['Study Satisfaction'].value_counts(dropna=False)\n",
    "#train_data['Academic Pressure'].value_counts(dropna=False)\n",
    "#train_data['CGPA'].value_counts(dropna=False)\n",
    "#train_data['Profession'].value_counts(dropna=False)\n",
    "#train_data['Work Pressure'].value_counts(dropna=False)\n",
    "#train_data['Job Satisfaction'].value_counts(dropna=False)\n",
    "#train_data['Dietary Habits'].value_counts(dropna=False)\n",
    "#train_data['Financial Stress'].value_counts(dropna=False)\n",
    "pd.set_option('display.max_rows', None)\n",
    "train_data['Degree'].value_counts(dropna=False)\n",
    "#regulated\n",
    "#Comclusion: all our missing vals are NANS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873306d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forçar a visualização completa das contagens\n",
    "pd.set_option('display.max_rows', None)  # Isso vai permitir que todos os valores sejam exibidos\n",
    "print(train_data['Profession'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660de5c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:58.823839Z",
     "start_time": "2025-05-12T14:02:58.761838Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['Profession'].value_counts(dropna=False)\n",
    "\n",
    "valid_professions = [\n",
    "    \"Teacher\", \"Content Writer\", \"Architect\", \"Consultant\", \"HR Manager\",\n",
    "    \"Pharmacist\", \"Doctor\", \"Business Analyst\", \"Entrepreneur\", \"Chemist\",\n",
    "    \"Chef\", \"Educational Consultant\", \"Data Scientist\", \"Researcher\", \"Lawyer\",\n",
    "    \"Customer Support\", \"Marketing Manager\", \"Pilot\", \"Travel Consultant\",\n",
    "    \"Plumber\", \"Sales Executive\", \"Manager\", \"Judge\", \"Electrician\",\n",
    "    \"Financial Analyst\", \"Software Engineer\", \"Civil Engineer\", \"UX/UI Designer\",\n",
    "    \"Digital Marketer\", \"Accountant\", \"Mechanical Engineer\", \"Graphic Designer\",\n",
    "    \"Research Analyst\", \"Investment Banker\", \"Analyst\", \"Academic\", \"Unemployed\", \"Medical Doctor\", \"City Manager\", \"Family Consultant\"\n",
    "]\n",
    "\n",
    "# corrigir erros digitação\n",
    "def correct_profession(value):\n",
    "    corrections = {\n",
    "        \"Finanancial Analyst\": \"Financial Analyst\",\n",
    "        # Adicionar mais ??\n",
    "    }\n",
    "    return corrections.get(value, value)\n",
    "\n",
    "train_data['Profession'] = train_data['Profession'].apply(correct_profession)\n",
    "\n",
    "test_data['Profession'] = test_data['Profession'].apply(correct_profession)\n",
    "\n",
    "def clean_profession(value):\n",
    "    if pd.isna(value):\n",
    "        return value  # mantém NaN\n",
    "    return value if value in valid_professions else \"other\"\n",
    "\n",
    "train_data['Profession'] = train_data['Profession'].apply(clean_profession)\n",
    "test_data['Profession'] = test_data['Profession'].apply(clean_profession)\n",
    "\n",
    "train_data['Profession'].value_counts(dropna=False)\n",
    "\n",
    "test_data['Profession'].value_counts(dropna=False)\n",
    "\n",
    "# these are names: [\"Yogesh\", \"Pranav\", \"Dev\", \"Yuvraj\"]\n",
    "# these seem to be localities [\"Patna\", \"Visakhapatnam\", \"Nagpur\", \"FamilyVirar\"]\n",
    "# and these ? what are hey ? not jobs. [\"Patna\", \"Visakhapatnam\", \"Nagpur\", \"FamilyVirar\"]\n",
    "#degrees like MBA\n",
    "#substringing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a72413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:59.400537Z",
     "start_time": "2025-05-12T14:02:58.906575Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# handle ranges like 6-8, handle more than/less than X,handle direct numeric values\n",
    "print(len(train_data['Sleep Duration']))\n",
    "#15 rows lost\n",
    "def normalize_sleep_duration(column):\n",
    "    def normalize(value):\n",
    "        value = str(value).strip()\n",
    "\n",
    "        match_range = re.match(r\"(\\d+)\\s*-\\s*(\\d+)\", value)\n",
    "        if match_range:\n",
    "            x, y = map(int, match_range.groups())\n",
    "            return (x + y) / 2\n",
    "\n",
    "        match_more = re.match(r\"More than (\\d+)\", value, re.IGNORECASE)\n",
    "        if match_more:\n",
    "            return int(match_more.group(1)) + 0.5\n",
    "\n",
    "        match_less = re.match(r\"Less than (\\d+)\", value, re.IGNORECASE)\n",
    "        if match_less:\n",
    "            return int(match_less.group(1)) - 0.5\n",
    "\n",
    "        try:\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            return pd.NA \n",
    "\n",
    "    return column.apply(normalize)\n",
    "\n",
    "def normalize_large_sleep_values(column):\n",
    "    def adjust_large(value):\n",
    "        try:\n",
    "            if pd.notna(value) and value >= 12:\n",
    "                return round(value / 7 * 2) / 2\n",
    "            return value\n",
    "        except:\n",
    "            return pd.NA\n",
    "    return column.apply(adjust_large)\n",
    "\n",
    "train_data['Sleep Duration'] = normalize_sleep_duration(train_data['Sleep Duration'])\n",
    "train_data['Sleep Duration'] = normalize_large_sleep_values(train_data['Sleep Duration'])\n",
    "test_data['Sleep Duration'] = normalize_sleep_duration(test_data['Sleep Duration'])\n",
    "test_data['Sleep Duration'] = normalize_large_sleep_values(test_data['Sleep Duration'])\n",
    "\n",
    "print(train_data['Sleep Duration'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d35ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:02:59.407590647Z",
     "start_time": "2025-05-12T13:59:33.294320Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_dietary = [\"Moderate\", \"Unhealthy\", \"Healthy\"]\n",
    "\n",
    "train_data[\"Dietary Habits\"] = train_data[\"Dietary Habits\"].apply(\n",
    "    lambda x: x if pd.isna(x) or x in valid_dietary else \"other\"\n",
    ")\n",
    "test_data[\"Dietary Habits\"] = test_data[\"Dietary Habits\"].apply(\n",
    "    lambda x: x if pd.isna(x) or x in valid_dietary else \"other\"\n",
    ")\n",
    "\n",
    "\n",
    "print(train_data['Dietary Habits'].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_degree(column):\n",
    "    def clean(item):\n",
    "        if isinstance(item, str):\n",
    "            item = item.replace('.', '').replace(' ', '')\n",
    "            return item\n",
    "        else:\n",
    "            return 'invalid'\n",
    "    \n",
    "    column = column.apply(clean)\n",
    "    def remove_names(item):\n",
    "            if (len(item) > 1 and item[0].isupper() and item[1].isupper() and item[0] in ['L', 'P', 'B', 'M']) or item == 'Class12' or item == \"PhD\":\n",
    "                return item\n",
    "            else:\n",
    "                return 'invalid' \n",
    "    return column.apply(remove_names)\n",
    "#importante dar NA aos inválidos para dar drop\n",
    "train_data['Degree'] = normalize_degree(train_data['Degree'])\n",
    "test_data['Degree'] = normalize_degree(test_data['Degree'])\n",
    "\n",
    "degree_counts = train_data['Degree'].value_counts()\n",
    "rare_degrees = degree_counts[degree_counts <= 5].index\n",
    "\n",
    "train_data['Degree'] = train_data['Degree'].apply(lambda x: 'other' if x in rare_degrees else x)\n",
    "test_data['Degree'] = test_data['Degree'].apply(lambda x: 'other' if x in rare_degrees else x)\n",
    "     \n",
    "\n",
    "print(train_data['Degree'].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d23e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data['Work/Study Hours'].value_counts(dropna=False))\n",
    "\"\"\"\n",
    "train_data['Work/Study Hours'].value_counts(dropna=False)\n",
    "train_data['Academic Pressure'].value_counts(dropna=False)\n",
    "train_data['CGPA'].value_counts(dropna=False)\n",
    "train_data['Profession'].value_counts(dropna=False)\n",
    "train_data['Work Pressure'].value_counts(dropna=False)\n",
    "train_data['Job Satisfaction'].value_counts(dropna=False)\n",
    "train_data['Dietary Habits'].value_counts(dropna=False)\n",
    "train_data['Financial Stress'].value_counts(dropna=False)\n",
    "train_data['Degree'].value_counts(dropna=False)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2b0a8",
   "metadata": {},
   "source": [
    "we can detect in our data two categories of individuals, identifiable by their attributes:\n",
    "- **Students:** academic pressure, CGPA, study satisfaction, degree\n",
    "- **Worker Professionals:** work pressure, profession, job satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec77f77",
   "metadata": {},
   "source": [
    "Our decision tree classifier requires binary values. Thus, let's convert bicategorical variables in to 0/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gender column: Male -> 1, Female -> 0\n",
    "train_data['Gender'] = train_data['Gender'].replace({'Male': 1, 'Female': 0})\n",
    "\n",
    "train_data['Working Professional or Student'] = train_data['Working Professional or Student'].replace({'Working Professional': 1, 'Student': 0})\n",
    "\n",
    "# Have you ever had suicidal thoughts?\n",
    "train_data['Have you ever had suicidal thoughts ?'] = train_data['Have you ever had suicidal thoughts ?'].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "train_data['Family History of Mental Illness'] = train_data['Family History of Mental Illness'].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "test_data['Gender'] = test_data['Gender'].replace({'Male': 1, 'Female': 0})\n",
    "\n",
    "test_data['Working Professional or Student'] = test_data['Working Professional or Student'].replace({'Working Professional': 1, 'Student': 0})\n",
    "\n",
    "test_data['Have you ever had suicidal thoughts ?'] = test_data['Have you ever had suicidal thoughts ?'].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "test_data['Family History of Mental Illness'] = test_data['Family History of Mental Illness'].replace({'Yes': 1, 'No': 0})\n",
    "#print(train_data['Gender']).value_counts().sort(ascending=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff83619",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Satisfaction'] = train_data[['Job Satisfaction', 'Study Satisfaction']].mean(axis=1, skipna=True)\n",
    "\n",
    "train_data = train_data.drop(columns=['Job Satisfaction', 'Study Satisfaction'])\n",
    "train_data['Pressure'] = train_data[['Work Pressure', 'Academic Pressure']].mean(axis=1, skipna=True)\n",
    "\n",
    "train_data = train_data.drop(columns=['Work Pressure', 'Academic Pressure'])\n",
    "print(train_data[['Pressure']].head())\n",
    "\n",
    "print(train_data[['Satisfaction']].head())\n",
    "\n",
    "# Preencher 'Profession' onde está vazia, com base na coluna \"Working Professional or Student\"\n",
    "train_data.loc[\n",
    "    train_data['Profession'].isna() & (train_data['Working Professional or Student'] == 0),\n",
    "    'Profession'\n",
    "] = 'Student'\n",
    "\n",
    "train_data.loc[\n",
    "    train_data['Profession'].isna() & (train_data['Working Professional or Student'] != 0),\n",
    "    'Profession'\n",
    "] = 'other'\n",
    "train_data = train_data.drop(columns=['CGPA'])\n",
    "print(train_data[['Profession']].head())\n",
    "\n",
    "test_data['Satisfaction'] = test_data[['Job Satisfaction', 'Study Satisfaction']].mean(axis=1, skipna=True)\n",
    "\n",
    "test_data = test_data.drop(columns=['Job Satisfaction', 'Study Satisfaction'])\n",
    "test_data['Pressure'] = test_data[['Work Pressure', 'Academic Pressure']].mean(axis=1, skipna=True)\n",
    "\n",
    "test_data = test_data.drop(columns=['Work Pressure', 'Academic Pressure'])\n",
    "\n",
    "\n",
    "# Preencher 'Profession' onde está vazia, com base na coluna \"Working Professional or Student\"\n",
    "test_data.loc[\n",
    "    test_data['Profession'].isna() & (test_data['Working Professional or Student'] == 0),\n",
    "    'Profession'\n",
    "] = 'Student'\n",
    "\n",
    "test_data.loc[\n",
    "    test_data['Profession'].isna() & (test_data['Working Professional or Student'] != 0),\n",
    "    'Profession'\n",
    "] = 'other'\n",
    "test_data = test_data.drop(columns=['CGPA'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"train_merge.csv\", index=False)\n",
    "test_data.to_csv(\"test_merge.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cece835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDERSAMPLING COM PRIORIDADE\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "X = train_data.drop(\"Depression\", axis=1)\n",
    "y = train_data[\"Depression\"]\n",
    "\n",
    "data = pd.concat([X, y], axis=1)\n",
    "\n",
    "class_counts = data[\"Depression\"].value_counts()\n",
    "min_class_size = class_counts.min()\n",
    "\n",
    "priority_cols = [\"Profession\", \"Degree\", \"Dietary Habits\"]\n",
    "\n",
    "balanced_data = []\n",
    "\n",
    "for label in class_counts.index:\n",
    "    subset = data[data[\"Depression\"] == label]\n",
    "\n",
    "    if len(subset) > min_class_size:\n",
    "        \n",
    "        to_keep = min_class_size\n",
    "        \n",
    "        na_rows = subset[priority_cols].isna().any(axis=1)\n",
    "        subset = subset[~na_rows]\n",
    "\n",
    "        # Primeira prioridade: linhas com \"invalid\"\n",
    "        invalid_rows = subset[subset[priority_cols].isin([\"invalid\"]).any(axis=1)]\n",
    "        subset = subset.drop(invalid_rows.index)\n",
    "\n",
    "        # Segunda prioridade: linhas com \"other\"\n",
    "        other_rows = subset[subset[priority_cols].isin([\"other\"]).any(axis=1)]\n",
    "        subset = subset.drop(other_rows.index)\n",
    "\n",
    "        remaining_needed = to_keep\n",
    "\n",
    "\n",
    "        if len(subset) >= remaining_needed:\n",
    "            to_sample = subset.sample(remaining_needed, random_state=42)\n",
    "        else:\n",
    "            # Remover todos os 'priority' e sortear os restantes\n",
    "            rows_needed = remaining_needed - len(subset)\n",
    "\n",
    "            if len(other_rows) >= rows_needed:\n",
    "                to_sample = pd.concat([subset, other_rows.sample(rows_needed, random_state=42)])\n",
    "            else:\n",
    "                still_needed = rows_needed - len(other_rows)\n",
    "                to_sample = pd.concat([\n",
    "                    subset,\n",
    "                    other_rows,\n",
    "                    invalid_rows.sample(still_needed, random_state=42)\n",
    "                ])\n",
    "    else:\n",
    "\n",
    "        to_sample = subset\n",
    "\n",
    "    balanced_data.append(to_sample)\n",
    "\n",
    "undersampled_data = pd.concat(balanced_data)\n",
    "\n",
    "undersampled_data.to_csv(\"final_train_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = undersampled_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd5e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[['Satisfaction']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"train_merge.csv\")\n",
    "print(df.columns)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1b45e",
   "metadata": {},
   "source": [
    "ALGORITHMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a030350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Copiar o dataset para n mudar dados do original\n",
    "data = df.copy()\n",
    "data_test = test_data.copy()\n",
    "\n",
    "# Remover colunas não relevantes\n",
    "data.drop(columns=['id', 'Name'], inplace=True)\n",
    "data_test.drop(columns=['id', 'Name'], inplace=True)\n",
    "\n",
    "categorical_cols_train = data.select_dtypes(include=['object']).columns #identifica todas as colunas com valores strings\n",
    "\n",
    "#categorical_cols_test = data_test.select_dtypes(include=['object']).columns #identifica todas as colunas com valores strings\n",
    "\n",
    "# transforma as colunas com \"palavras\" em numeros\n",
    "label_encoders = {}\n",
    "for col in categorical_cols_train:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "for col in categorical_cols_train:\n",
    "    if col in data_test.columns:\n",
    "        le = label_encoders[col]\n",
    "        # Substituir valores desconhecidos por 'unknown'\n",
    "        data_test[col] = data_test[col].apply(lambda x: x if x in le.classes_ else 'unknown')\n",
    "\n",
    "        # Expandir os classes_ com 'unknown' (LabelEncoder precisa disto!)\n",
    "        if 'unknown' not in le.classes_:\n",
    "            le.classes_ = np.append(le.classes_, 'unknown')\n",
    "\n",
    "        data_test[col] = le.transform(data_test[col].astype(str))\n",
    "\n",
    "\n",
    "X_train = data.drop(columns='Depression') #features para determinar a depressao\n",
    "y_train = data['Depression'] #alvo\n",
    "\n",
    "X_test = data_test[X_train.columns]\n",
    "\n",
    "\n",
    "# Tratar valores nulos (estou a por a média quando tem valores nulos)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_data_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "# Converter todas as colunas para numérico\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Agora aplicar imputação\n",
    "test_data_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "#print(X_test.dtypes)\n",
    "#print(X_test.isna().sum())  # ver se ainda há NaNs\n",
    "\n",
    "\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_trainSplit, X_testSplit, y_trainSplit, y_testSplit = train_test_split(train_data_imputed, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Padronizar os dados\n",
    "#Este é com o split para fazermos as métricas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_trainSplit)\n",
    "X_test_scaled = scaler.transform(X_testSplit)\n",
    "\n",
    "\n",
    "X_train_scaled[:5]\n",
    "\n",
    "# Padronizar os dados (por tudo na mesma escala : nao pode idade(0-100) e genero(0 ou 1), é preciso escalar)\n",
    "# ESte é para os dados de teste real que queremos tentar prever\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_real = scaler.fit_transform(train_data_imputed)\n",
    "X_test_scaled_real = scaler.transform(test_data_imputed)\n",
    "\n",
    "#X_train_scaled[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88bfca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Instanciar e treinar o modelo\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_trainSplit)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Avaliar o desempenho\n",
    "accuracy = accuracy_score(y_testSplit, y_pred)\n",
    "conf_matrix = confusion_matrix(y_testSplit, y_pred)\n",
    "class_report = classification_report(y_testSplit, y_pred)\n",
    "\n",
    "class_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cd61a",
   "metadata": {},
   "source": [
    "graphics / confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8dd162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Depression', 'Depression'],\n",
    "            yticklabels=['No Depression', 'Depression'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - KNN')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "y_prob = knn.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_testSplit, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=f'KNN (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - KNN')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee28b6",
   "metadata": {},
   "source": [
    "variação valor k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Testar vários valores de k (1 a 20)\n",
    "k_values = range(1, 11)\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_k = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_k.fit(X_train_scaled, y_trainSplit)\n",
    "    y_pred_k = knn_k.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_testSplit, y_pred_k)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "# Plot da acurácia vs. número de vizinhos (k)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, accuracies, marker='o')\n",
    "plt.title('KNN Accuracy for Different Values of k')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572cad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calcular métricas individualmente\n",
    "precision_0 = precision_score(y_testSplit, y_pred, pos_label=0)\n",
    "recall_0 = recall_score(y_testSplit, y_pred, pos_label=0)\n",
    "f1_0 = f1_score(y_testSplit, y_pred, pos_label=0)\n",
    "\n",
    "precision_1 = precision_score(y_testSplit, y_pred, pos_label=1)\n",
    "recall_1 = recall_score(y_testSplit, y_pred, pos_label=1)\n",
    "f1_1 = f1_score(y_testSplit, y_pred, pos_label=1)\n",
    "\n",
    "# Criar DataFrame com as métricas\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': ['No Depression (0)', 'Depression (1)'],\n",
    "    'Precision': [precision_0, precision_1],\n",
    "    'Recall': [recall_0, recall_1],\n",
    "    'F1-score': [f1_0, f1_1],\n",
    "    'Support': [conf_matrix[0, 0] + conf_matrix[0, 1], conf_matrix[1, 0] + conf_matrix[1, 1]]\n",
    "})\n",
    "\n",
    "print(metrics_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e06b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tentar adivinhar os reais\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Instanciar e treinar o modelo\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled_real, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = knn.predict(X_test_scaled_real)\n",
    "\n",
    "# Mostrar as previsões\n",
    "print(\"Previsões do modelo:\")\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
